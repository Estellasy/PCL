{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/siyi/project/PCL/pcl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import builtins\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = torch.rand(size=(1, 256, 80, 80))\n",
    "p2 = torch.rand(size=(1, 512, 40, 40))\n",
    "p3 = torch.rand(size=(1, 1024, 20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = nn.functional.adaptive_avg_pool2d(p1, (20, 20))  # torch.Size([1, 256, 20, 20])\n",
    "p2 = nn.functional.adaptive_avg_pool2d(p2, (20, 20))  # torch.Size([1, 512, 20, 20])\n",
    "fused = torch.cat((p1, p2, p3), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1792, 20, 20])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fused.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> creating model 'resnet50'\n"
     ]
    }
   ],
   "source": [
    "print(\"=> creating model '{}'\".format(\"resnet50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@Description :\n",
    "@Author      : siyiren1@foxmail.com\n",
    "@Time        : 2024/07/21 15:33:48\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from random import sample\n",
    "from neck import Yolov8Neck\n",
    "\n",
    "\n",
    "class DetectionCL(nn.Module):\n",
    "    \"\"\"\n",
    "    Build a DetectionCL model, change the MLP layer into detection layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_encoder, head=None, dim=128, r=10, m=0.999, T=0.1, loss_lambda=0.5,  mlp=True) -> None:\n",
    "        super(DetectionCL, self).__init__()\n",
    "\n",
    "        self.r, self.m, self.T = r, m, T\n",
    "\n",
    "        # 创建编码器 其中num_classes=dim是fc层的输出维度\n",
    "        self.encoder_q = nn.Sequential(\n",
    "            base_encoder(num_classes=dim),\n",
    "            nn.Sequential()\n",
    "        )\n",
    "        self.encoder_k = nn.Sequential(\n",
    "            base_encoder(num_classes=dim),\n",
    "            nn.Sequential()\n",
    "        )\n",
    "\n",
    "        # 硬编码mlp层\n",
    "        if mlp:\n",
    "            dim_mlp = self.encoder_q[0].fc.weight.shape[1]\n",
    "            # 删除原avgpool/fc层 并替换mlp\n",
    "            self.encoder_q[0].avgpool = nn.Identity()\n",
    "            self.encoder_q[0].avgpool = nn.Identity()\n",
    "            self.encoder_q[0].fc = nn.Identity()\n",
    "            self.encoder_k[0].avgpool = nn.Identity()\n",
    "            self.encoder_k[0].fc = nn.Identity()\n",
    "\n",
    "            # 更新neck\n",
    "            # self.encoder_q[1] = Yolov8Neck()\n",
    "            # self.encoder_k[1] = Yolov8Neck()\n",
    "\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data.copy_(param_q.data)  # 初始化encoder_k的参数为encoder_q的参数\n",
    "            param_k.requires_grad = False  # encoder_k不进行梯度更新\n",
    "\n",
    "        # 创建两个队列 分别为global和dense\n",
    "        self.register_buffer(\"queue\", torch.randn(dim, r))\n",
    "        self.queue = nn.functional.normalize(self.queue, dim=0)\n",
    "        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "        self.register_buffer(\"queue2\",  torch.randn(dim, r))\n",
    "        self.queue2 = nn.functional.normalize(self.queue2, dim=0)\n",
    "        self.register_buffer(\"queue2_ptr\", torch.zeros(1, dtype=torch.long))\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _momentum_update_key_encoder(self):\n",
    "        \"\"\"\n",
    "        Momentum update of the key encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):\n",
    "            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue(self, keys):\n",
    "        # gather keys before updating queue\n",
    "        keys = concat_all_gather(keys)\n",
    "\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue_ptr)\n",
    "        assert self.queue_len % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue[:, ptr:ptr + batch_size] = keys.transpose(0, 1)\n",
    "        ptr = (ptr + batch_size) % self.queue_len  # move pointer\n",
    "\n",
    "        self.queue_ptr[0] = ptr\n",
    "\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def _dequeue_and_enqueue2(self, keys):\n",
    "        # gather keys before updating queue\n",
    "        keys = concat_all_gather(keys)\n",
    "\n",
    "        batch_size = keys.shape[0]\n",
    "\n",
    "        ptr = int(self.queue2_ptr)\n",
    "        assert self.queue_len % batch_size == 0  # for simplicity\n",
    "\n",
    "        # replace the keys at ptr (dequeue and enqueue)\n",
    "        self.queue2[:, ptr:ptr + batch_size] = keys.transpose(0, 1)\n",
    "        ptr = (ptr + batch_size) % self.queue_len  # move pointer\n",
    "\n",
    "        self.queue2_ptr[0] = ptr\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_shuffle_ddp(self, x):\n",
    "        \"\"\"\n",
    "        Batch shuffle, for making use of BatchNorm.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # random shuffle index\n",
    "        idx_shuffle = torch.randperm(batch_size_all).cuda()\n",
    "\n",
    "        # broadcast to all gpus\n",
    "        torch.distributed.broadcast(idx_shuffle, src=0)\n",
    "\n",
    "        # index for restoring\n",
    "        idx_unshuffle = torch.argsort(idx_shuffle)\n",
    "\n",
    "        # shuffled index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this], idx_unshuffle\n",
    "    \n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _batch_unshuffle_ddp(self, x, idx_unshuffle):\n",
    "        \"\"\"\n",
    "        Undo batch shuffle.\n",
    "        *** Only support DistributedDataParallel (DDP) model. ***\n",
    "        \"\"\"\n",
    "        # gather from all gpus\n",
    "        batch_size_this = x.shape[0]\n",
    "        x_gather = concat_all_gather(x)\n",
    "        batch_size_all = x_gather.shape[0]\n",
    "\n",
    "        num_gpus = batch_size_all // batch_size_this\n",
    "\n",
    "        # restored index for this gpu\n",
    "        gpu_idx = torch.distributed.get_rank()\n",
    "        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]\n",
    "\n",
    "        return x_gather[idx_this]\n",
    "    \n",
    "\n",
    "    def forward(self, im_q, im_k=None, is_eval=False, cluster_global=None, cluster_dense=None, index=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_encoderq_features(self, im_q):\n",
    "        features = im_q\n",
    "        output_list = []\n",
    "        for name, layer in self.encoder_q[0].named_children():\n",
    "            if name == 'avgpool':\n",
    "                break\n",
    "            features = layer(features)\n",
    "            if name.startswith(\"layer\"):\n",
    "                output_list.append(features)\n",
    "        return output_list, features\n",
    "\n",
    "\n",
    "# utils\n",
    "@torch.no_grad()\n",
    "def concat_all_gather(tensor):\n",
    "    \"\"\"\n",
    "    Performs all_gather operation on the provided tensors.\n",
    "    *** Warning ***: torch.distributed.all_gather has no gradient.\n",
    "    \"\"\"\n",
    "    tensors_gather = [torch.ones_like(tensor)\n",
    "        for _ in range(torch.distributed.get_world_size())]\n",
    "    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n",
    "\n",
    "    output = torch.cat(tensors_gather, dim=0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_encoder = models.__dict__[\"resnet50\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetectionCL(base_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(1, 3, 640, 640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list, features = model.get_encoderq_features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.encoder_q[0](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 160, 160])\n",
      "torch.Size([1, 512, 80, 80])\n",
      "torch.Size([1, 1024, 40, 40])\n",
      "torch.Size([1, 2048, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "for f in output_list:\n",
    "    print(f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2048, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics.nn.modules.block import C2f\n",
    "\n",
    "class Yolov8Head(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Yolov8Head, self).__init__()\n",
    "        # channels is a list of input channels for each stage\n",
    "        c1, c2, c3, c4 = channels\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f_1 = C2f(c3 + c4, 512)\n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f_2 = C2f(512 + c2, 256)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.c2f_3 = C2f(256 + c2, 512)\n",
    "        self.conv3 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.c2f_4 = C2f(512 + c4, 1024)\n",
    "\n",
    "        # FPN layers\n",
    "        self.lateral_conv1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral_conv2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral_conv3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)        \n",
    "\n",
    "        self.smooth_conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth_conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth_conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        assert len(input_list) == 4\n",
    "        x4, x3, x2, x1 = input_list\n",
    "\n",
    "        x = self.up1(x1)    # torch.Size([1, 2048, 40, 40])\n",
    "        print(\"up: \", x.shape)\n",
    "        x = torch.cat((x, x2), dim=1) # torch.Size([1, 3072, 40, 40])\n",
    "        print(\"concat: \", x.shape)\n",
    "        x = self.c2f_1(x)   # torch.Size([1, 512, 40, 40])\n",
    "        print(\"c2f: \", x.shape)\n",
    "        hidden_x = x\n",
    "\n",
    "        x = self.up2(x)\n",
    "        print(\"up: \", x.shape)\n",
    "        x = torch.cat((x, x3), dim=1)\n",
    "        print(\"concat: \", x.shape)\n",
    "        x = self.c2f_2(x)\n",
    "        p1 = x\n",
    "        print(\"c2f: \", x.shape)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        print(\"conv: \", x.shape)\n",
    "        x = torch.cat((x, hidden_x), dim=1)\n",
    "        print(\"concat: \", x.shape)\n",
    "        x = self.c2f_3(x)\n",
    "        p2 = x\n",
    "        print(\"c2f: \", x.shape)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        print(\"conv: \", x.shape)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        print(\"concat: \", x.shape)\n",
    "        x = self.c2f_4(x)\n",
    "        p3 = x\n",
    "        print(\"c2f: \", x.shape)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up:  torch.Size([1, 2048, 40, 40])\n",
      "concat:  torch.Size([1, 3072, 40, 40])\n",
      "c2f:  torch.Size([1, 512, 40, 40])\n",
      "up:  torch.Size([1, 512, 80, 80])\n",
      "concat:  torch.Size([1, 1024, 80, 80])\n",
      "c2f:  torch.Size([1, 256, 80, 80])\n",
      "conv:  torch.Size([1, 256, 40, 40])\n",
      "concat:  torch.Size([1, 768, 40, 40])\n",
      "c2f:  torch.Size([1, 512, 40, 40])\n",
      "conv:  torch.Size([1, 512, 20, 20])\n",
      "concat:  torch.Size([1, 2560, 20, 20])\n",
      "c2f:  torch.Size([1, 1024, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "# channels = [feature.shape[1] for feature in output_list]\n",
    "# model = Yolov8Head(channels)\n",
    "channels = [256, 512, 1024, 2048]\n",
    "model = Yolov8Head(channels)\n",
    "input_list = [\n",
    "    torch.randn(1, 256, 160, 160),\n",
    "    torch.randn(1, 512, 80, 80),\n",
    "    torch.randn(1, 1024, 40, 40),\n",
    "    torch.randn(1, 2048, 20, 20)\n",
    "]\n",
    "output = model(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2f:  torch.Size([1, 256, 80, 80])\n",
    "c2f:  torch.Size([1, 512, 40, 40])\n",
    "c2f:  torch.Size([1, 1024, 20, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 20, 20])\n",
      "torch.Size([1, 512, 40, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [256, 1024, 1, 1], expected input[1, 512, 40, 40] to have 1024 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 86\u001b[0m\n\u001b[1;32m     79\u001b[0m model \u001b[38;5;241m=\u001b[39m Yolov8Head(channels)\n\u001b[1;32m     80\u001b[0m input_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m),\n\u001b[1;32m     82\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m80\u001b[39m),\n\u001b[1;32m     83\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m40\u001b[39m),\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     85\u001b[0m ]\n\u001b[0;32m---> 86\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape will depend on the final FPN output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/detection/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[104], line 65\u001b[0m, in \u001b[0;36mYolov8Head.forward\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(p2\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     64\u001b[0m p3_upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(p3)   \u001b[38;5;66;03m# Upsample to match p2\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlateral_conv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m p3_upsampled\n\u001b[1;32m     66\u001b[0m p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_conv1(p2)\n\u001b[1;32m     68\u001b[0m p2_upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(p2)   \u001b[38;5;66;03m# Upsample to match p1\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/detection/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/detection/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/detection/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [256, 1024, 1, 1], expected input[1, 512, 40, 40] to have 1024 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics.nn.modules.block import C2f\n",
    "\n",
    "class Yolov8Head(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Yolov8Head, self).__init__()\n",
    "        # channels is a list of input channels for each stage\n",
    "        c1, c2, c3, c4 = channels\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f_1 = C2f(c3 + c4, 512)\n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f_2 = C2f(512 + c2, 256)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.c2f_3 = C2f(256 + c2, 512)\n",
    "        self.conv3 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.c2f_4 = C2f(512 + c4, 1024)\n",
    "\n",
    "        # FPN layers\n",
    "        self.lateral_conv1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral_conv2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral_conv3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)        \n",
    "\n",
    "        self.smooth_conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth_conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth_conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Output layers for final feature maps\n",
    "        self.output_conv = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        assert len(input_list) == 4\n",
    "        x4, x3, x2, x1 = input_list\n",
    "\n",
    "        # Bottom-up pathway\n",
    "        x = self.up1(x1)    # torch.Size([1, 2048, 40, 40])\n",
    "        x = torch.cat((x, x2), dim=1) # torch.Size([1, 3072, 40, 40])\n",
    "        x = self.c2f_1(x)   # torch.Size([1, 512, 40, 40])\n",
    "        hidden_x = x\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat((x, x3), dim=1)\n",
    "        x = self.c2f_2(x)\n",
    "        p1 = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat((x, hidden_x), dim=1)\n",
    "        x = self.c2f_3(x)\n",
    "        p2 = x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.c2f_4(x)\n",
    "        p3 = x\n",
    "\n",
    "        # Top-down pathway and lateral connections\n",
    "        print(p3.shape)\n",
    "        print(p2.shape)\n",
    "        p3_upsampled = self.up1(p3)   # Upsample to match p2\n",
    "        p2 = self.lateral_conv1(p2) + p3_upsampled\n",
    "        p2 = self.smooth_conv1(p2)\n",
    "\n",
    "        p2_upsampled = self.up1(p2)   # Upsample to match p1\n",
    "        p1 = self.lateral_conv2(p1) + p2_upsampled\n",
    "        p1 = self.smooth_conv2(p1)\n",
    "\n",
    "        # Final output\n",
    "        p1 = self.smooth_conv3(p1)\n",
    "\n",
    "        return p1\n",
    "\n",
    "# Example usage:\n",
    "channels = [256, 512, 1024, 2048]\n",
    "model = Yolov8Head(channels)\n",
    "input_list = [\n",
    "    torch.randn(1, 256, 160, 160),\n",
    "    torch.randn(1, 512, 80, 80),\n",
    "    torch.randn(1, 1024, 40, 40),\n",
    "    torch.randn(1, 2048, 20, 20)\n",
    "]\n",
    "output = model(input_list)\n",
    "print(output.shape)  # Expected output shape will depend on the final FPN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p3:  torch.Size([1, 1024, 20, 20])\n",
      "p2:  torch.Size([1, 512, 40, 40])\n",
      "p3_upsampled:  torch.Size([1, 1024, 40, 40])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (1024) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 89\u001b[0m\n\u001b[1;32m     82\u001b[0m model \u001b[38;5;241m=\u001b[39m Yolov8Head(channels)\n\u001b[1;32m     83\u001b[0m input_list \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     84\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m160\u001b[39m, \u001b[38;5;241m160\u001b[39m),\n\u001b[1;32m     85\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m80\u001b[39m),\n\u001b[1;32m     86\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m40\u001b[39m),\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2048\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     88\u001b[0m ]\n\u001b[0;32m---> 89\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Expected output shape will depend on the final FPN output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/detection/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[105], line 67\u001b[0m, in \u001b[0;36mYolov8Head.forward\u001b[0;34m(self, input_list)\u001b[0m\n\u001b[1;32m     65\u001b[0m p3_upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(p3)   \u001b[38;5;66;03m# Upsample to match p2\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mp3_upsampled: \u001b[39m\u001b[38;5;124m\"\u001b[39m, p3_upsampled\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 67\u001b[0m p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlateral_conv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp3_upsampled\u001b[49m\n\u001b[1;32m     68\u001b[0m p2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_conv1(p2)\n\u001b[1;32m     70\u001b[0m p2_upsampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup1(p2)   \u001b[38;5;66;03m# Upsample to match p1\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (1024) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics.nn.modules.block import C2f\n",
    "\n",
    "class Yolov8Head(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Yolov8Head, self).__init__()\n",
    "        # channels is a list of input channels for each stage\n",
    "        c1, c2, c3, c4 = channels\n",
    "\n",
    "        self.up1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f_1 = C2f(c3 + c4, 512)\n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.c2f_2 = C2f(512 + c2, 256)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.c2f_3 = C2f(256 + c2, 512)\n",
    "        self.conv3 = nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.c2f_4 = C2f(512 + c4, 1024)\n",
    "\n",
    "        # FPN layers\n",
    "        self.lateral_conv1 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral_conv2 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.lateral_conv3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)        \n",
    "\n",
    "        self.smooth_conv1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth_conv2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth_conv3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Output layers for final feature maps\n",
    "        self.output_conv = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        assert len(input_list) == 4\n",
    "        x4, x3, x2, x1 = input_list\n",
    "\n",
    "        # Bottom-up pathway\n",
    "        x = self.up1(x1)    # torch.Size([1, 2048, 40, 40])\n",
    "        x = torch.cat((x, x2), dim=1) # torch.Size([1, 3072, 40, 40])\n",
    "        x = self.c2f_1(x)   # torch.Size([1, 512, 40, 40])\n",
    "        hidden_x = x\n",
    "\n",
    "        x = self.up2(x)\n",
    "        x = torch.cat((x, x3), dim=1)\n",
    "        x = self.c2f_2(x)\n",
    "        p1 = x\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = torch.cat((x, hidden_x), dim=1)\n",
    "        x = self.c2f_3(x)\n",
    "        p2 = x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = torch.cat((x, x1), dim=1)\n",
    "        x = self.c2f_4(x)\n",
    "        p3 = x\n",
    "\n",
    "        print(\"p3: \", p3.shape)\n",
    "        print(\"p2: \", p2.shape)\n",
    "\n",
    "        # Top-down pathway and lateral connections\n",
    "        p3_upsampled = self.up1(p3)   # Upsample to match p2\n",
    "        print(\"p3_upsampled: \", p3_upsampled.shape)\n",
    "        p2 = self.lateral_conv1(p2) + p3_upsampled\n",
    "        p2 = self.smooth_conv1(p2)\n",
    "\n",
    "        p2_upsampled = self.up1(p2)   # Upsample to match p1\n",
    "        print(\"p2_upsampled: \", p2_upsampled.shape)\n",
    "        p1 = self.lateral_conv2(p1) + p2_upsampled\n",
    "        p1 = self.smooth_conv2(p1)\n",
    "\n",
    "        # Final output\n",
    "        p1 = self.smooth_conv3(p1)\n",
    "\n",
    "        return p1\n",
    "\n",
    "# Example usage:\n",
    "channels = [256, 512, 1024, 2048]\n",
    "model = Yolov8Head(channels)\n",
    "input_list = [\n",
    "    torch.randn(1, 256, 160, 160),\n",
    "    torch.randn(1, 512, 80, 80),\n",
    "    torch.randn(1, 1024, 40, 40),\n",
    "    torch.randn(1, 2048, 20, 20)\n",
    "]\n",
    "output = model(input_list)\n",
    "print(output.shape)  # Expected output shape will depend on the final FPN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def euclidean_dist(x, y):  # x:[n, d]  y:[m, d]\n",
    "  xx = (x*x).sum(dim=1, keepdim=True)  # (n, 1)\n",
    "  yy = (y*y).sum(dim=1, keepdim=True).transpose(0, 1)  # (1, m)\n",
    "  xy = torch.mm(x, y.transpose(0, 1))  # (n, m)\n",
    "  return xx - 2*xy + yy\n",
    "\n",
    "\n",
    "def cosin_sim(x, y):  # x:[n, d]  y:[m, d]\n",
    "  x = x.unsqueeze(1)  # [n, 1, d]\n",
    "  y = y.unsqueeze(0)  # [1, m, d]\n",
    "  return F.cosine_similarity(x, y, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.rand(size=(4, 2, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.transpose(0, 1).reshape(-1, *features.size()[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 32, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = (logits/np.sqrt(features.size(1))).softmax(1)  # [N, C]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPN_WCP(nn.Module):  # CPN + Wasserstein Confidence Penalty\n",
    "  def __init__(self, backbone, aug_num, gamma):\n",
    "    super(CPN_WCP, self).__init__()\n",
    "    self.feature = backbone()\n",
    "    self.aug_num = aug_num\n",
    "    self.gamma = gamma\n",
    "\n",
    "  # def cuda(self):\n",
    "  #   self.feature = nn.DataParallel(self.feature, device_ids=[0, 1, 2, 3]).cuda()\n",
    "  #   return self\n",
    "\n",
    "  def forward(self, x, temp=5.):  # x:[batch, aug_num, C, H, W]\n",
    "    batch = x.size(0)\n",
    "    x = x.transpose(0, 1).reshape(-1, *x.size()[-3:])  # [aug_num*batch, C, H, W]\n",
    "    features = self.feature(x)  # [aug_num*batch, d]\n",
    "    eudis = euclidean_dist(features, features)  # [aug_num*batch, aug_num*batch]\n",
    "    logits = -eudis.reshape(self.aug_num*batch*self.aug_num, batch)  # [aug_num*batch*aug_num, batch]\n",
    "    # Cross Entropy Loss\n",
    "    targets = torch.from_numpy(np.repeat(range(batch), self.aug_num)).repeat(self.aug_num).cuda()  # [aug_num*batch*aug_num]\n",
    "    ce_loss = nn.CrossEntropyLoss()(logits/temp, targets)\n",
    "    # Wasserstein Distance Regularization\n",
    "    probs = (logits/np.sqrt(features.size(1))).softmax(1)  # [N, C]\n",
    "    target_probs = torch.ones(probs.size()).cuda()/batch  # [N, C]\n",
    "    with torch.no_grad():\n",
    "      features = features.reshape(self.aug_num, batch, -1).mean(0)  # [batch, d]\n",
    "      cost = 1.-cosin_sim(features, features)  # [batch, batch]\n",
    "      cost = (cost-cost.min(-1, keepdims=True)[0])/(cost.max(-1, keepdims=True)[0]-cost.min(-1, keepdims=True)[0])\n",
    "      cost = (self.gamma*cost+torch.eye(batch).cuda()).unsqueeze(0).repeat(probs.size(0), 1, 1)\n",
    "    wcp_loss = self.SinkhornDistance(probs, target_probs, cost)\n",
    "\n",
    "    loss = ce_loss + wcp_loss\n",
    "    return loss\n",
    "\n",
    "  def M(self, C, u, v, eps):\n",
    "    \"Modified cost for logarithmic updates\"\n",
    "    return (-C+u.unsqueeze(-1)+v.unsqueeze(-2))/eps\n",
    "\n",
    "  def SinkhornDistance(self, p1, p2, C, itr=5, eps=0.5):\n",
    "    u = torch.zeros_like(p1)\n",
    "    v = torch.zeros_like(p2)\n",
    "    for _ in range(itr):\n",
    "      u = eps*(torch.log(p1+1e-12)-torch.logsumexp(self.M(C, u, v, eps), dim=-1)) + u\n",
    "      v = eps*(torch.log(p2+1e-12)-torch.logsumexp(self.M(C, u, v, eps).transpose(-2, -1), dim=-1)) + v\n",
    "\n",
    "    pi = torch.exp(self.M(C, u, v, eps))\n",
    "    return (pi*C).sum((-2, -1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
